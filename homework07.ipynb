{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "id": "U1lUFaCWW_9l",
    "outputId": "4c03661b-e646-43b1-8b32-2b8d6f00dd30",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Num GPUs Available:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# disable compiler warnings\n",
    "import os\n",
    "\n",
    "# imports \n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "np_config.enable_numpy_behavior()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from typing import List\n",
    "import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'  # FATAL\n",
    "print(\"Num GPUs Available: \", tf.config.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "id": "uk6WUyODW_9n"
   },
   "outputs": [],
   "source": [
    "(train_ds, val_ds), ds_info = tfds.load('mnist', split=['train', 'test'], as_supervised=True, with_info=True)\n",
    "\n",
    "#tfds.show_examples(train_ds, ds_info)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "class CustomLSTM(tf.keras.layers.AbstractRNNCell):\n",
    "  def __init__(self, units, **kwargs):\n",
    "      self.units = units\n",
    "      super(CustomLSTM, self).__init__(**kwargs)\n",
    "\n",
    "      initializer = tf.keras.initializers.Orthogonal()\n",
    "\n",
    "      self.layer_information_eraser = tf.keras.layers.Dense(self.units, activation='sigmoid', kernel_initializer=initializer)\n",
    "      self.layer_new_information_filter = tf.keras.layers.Dense(self.units, activation='sigmoid', kernel_initializer=initializer)\n",
    "      self.layer_new_information = tf.keras.layers.Dense(self.units, activation='tanh', kernel_initializer=initializer)\n",
    "      self.layer_information_transfer_filter = tf.keras.layers.Dense(self.units, activation='sigmoid', kernel_initializer=initializer)\n",
    "\n",
    "  @property\n",
    "  def state_size(self):\n",
    "    return (self.units, self.units)\n",
    "    #return [tf.TensorShape([self.units]), tf.TensorShape([self.units])]\n",
    "\n",
    "  @property\n",
    "  def output_size(self):\n",
    "    return self.units\n",
    "\n",
    "  #def get_initial_state(self, inputs=None, batch_size=None, dtype=None):\n",
    "    #return [tf.zeros([self.units], tf.float32), tf.zeros([self.units], tf.float32)]\n",
    "\n",
    "  def call(self, inputs, states):\n",
    "    hidden_state, cell_state = states\n",
    "\n",
    "    hidden_input = tf.concat([inputs, hidden_state], 1)\n",
    "\n",
    "    cell_state = tf.math.multiply(cell_state, self.layer_information_eraser(hidden_input))\n",
    "    cell_state = tf.math.add(cell_state, tf.math.multiply(self.layer_new_information(hidden_input), self.layer_new_information_filter(hidden_input)))\n",
    "\n",
    "    hidden_state = tf.math.multiply(tf.math.tanh(cell_state), self.layer_information_transfer_filter(hidden_input))\n",
    "\n",
    "    return hidden_state, [hidden_state, cell_state]"
   ],
   "metadata": {
    "id": "DEFe60MFOII0"
   },
   "execution_count": 166,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "id": "jVBPWYKzW_9n"
   },
   "outputs": [],
   "source": [
    "class BasicConv(tf.keras.Model):\n",
    "    def __init__(self, seq_size, optimizer=tf.keras.optimizers.Adam()):\n",
    "        super(BasicConv, self).__init__()\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "        #self.loss_function = tf.keras.losses.CategoricalCrossentropy()\n",
    "        #self.metrics_list = [[tf.keras.metrics.CategoricalAccuracy(name=\"test_accuracy\"),\n",
    "        #                     tf.keras.metrics.Mean(name=\"test_loss\"),\n",
    "        #                     tf.keras.metrics.Mean(name=\"test_frob_norm\")],\n",
    "        #                     [tf.keras.metrics.CategoricalAccuracy(name=\"train_accuracy\"),\n",
    "        #                     tf.keras.metrics.Mean(name=\"train_loss\"),\n",
    "        #                     tf.keras.metrics.Mean(name=\"train_frob_norm\")]]\n",
    "\n",
    "        #self.metrics_list = [tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
    "        #                     tf.keras.metrics.Mean(name=\"loss\"),\n",
    "        #                     tf.keras.metrics.Mean(name=\"frob_norm\")]\n",
    "\n",
    "        output_size = (int) (9*seq_size+1)\n",
    "  \n",
    "        self.pooling = tf.keras.layers.MaxPooling2D()\n",
    "        self.my_layers = [\n",
    "                        tf.keras.layers.Conv2D(filters=16, kernel_size=3, padding='same', activation='relu', input_shape=(28, 28, 1)),\n",
    "                        tf.keras.layers.Conv2D(filters=16, kernel_size=3, padding='same', activation='relu', input_shape=(28, 28, 1)),\n",
    "                        tf.keras.layers.TimeDistributed(tf.keras.layers.MaxPooling2D()),\n",
    "                        tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding='same', activation='relu', input_shape=(14, 14, 1)),\n",
    "                        tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding='same', activation='relu', input_shape=(14, 14, 1)),\n",
    "                        tf.keras.layers.TimeDistributed(tf.keras.layers.GlobalAvgPool2D()),\n",
    "                        #tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(10, activation='softmax')),\n",
    "                        #tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(1, activation='relu')),\n",
    "                        tf.keras.layers.RNN(CustomLSTM(64), unroll=True, return_sequences=True),\n",
    "                        #tf.keras.layers.LSTM(20, unroll=True, return_sequences=True),\n",
    "                        #tf.keras.layers.Dense(output_size, activation='softmax')\n",
    "                        tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(output_size, activation='softmax'))\n",
    "                        ]\n",
    "\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, x, training=False):\n",
    "        x = self.my_layers[0](x)\n",
    "        x = self.my_layers[1](x)\n",
    "        x = self.my_layers[2](x)\n",
    "        x = self.my_layers[3](x)\n",
    "        x = self.my_layers[4](x)\n",
    "        x = self.my_layers[5](x)\n",
    "        x = self.my_layers[6](x)\n",
    "        x = self.my_layers[7](x)\n",
    "        #x = self.my_layers[8](x)\n",
    "        #x = tf.round(x)\n",
    "        \n",
    "        #for layer in self.my_layers:\n",
    "        #    tf.print(x)\n",
    "        #    x = layer(x)\n",
    "        return x\n",
    "\n",
    "    @tf.function\n",
    "    def compute_frobenius(self):\n",
    "        frobenius_norm = tf.zeros((1,))\n",
    "        for var in self.trainable_variables:\n",
    "            frobenius_norm += tf.norm(var, ord=\"euclidean\")\n",
    "        return frobenius_norm\n",
    "\n",
    "    # 3. metrics property\n",
    "    #\"\"\"@property\"\"\"\n",
    "    #def metrics(self):\n",
    "    #    return self.metrics_list\n",
    "\n",
    "    # 4. reset all metrics objects\n",
    "    #def reset_metrics(self):\n",
    "    #    for metric in self.metrics:\n",
    "    #      #for metric in metric_list:\n",
    "    #      metric.reset_states()\n",
    "\n",
    "    \"\"\"\n",
    "    # train_step method\n",
    "    @tf.function\n",
    "    def train_step(self, data):\n",
    "        img, label = data\n",
    "        \n",
    "        # compute output and loss, train the variables\n",
    "        with tf.GradientTape() as tape:\n",
    "            output = self(img, training=True)\n",
    "            loss = self.loss_function(label, output)\n",
    "            \n",
    "        # update trainable variables\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "\n",
    "        # update metrics\n",
    "        self.metrics_list[1][0].update_state(tf.argmax(output, axis=1), tf.argmax(label, axis=1))\n",
    "        self.metrics_list[1][1].update_state(loss)\n",
    "        self.metrics_list[1][2].update_state(self.compute_frobenius())\n",
    "        \n",
    "        # return a dict with metric information\n",
    "        return {m.name : m.result() for m in self.metrics_list[1]}\n",
    "\n",
    "\n",
    "\n",
    "    # test_step method\n",
    "    @tf.function\n",
    "    def test_step(self, data):\n",
    "        img, label = data\n",
    "\n",
    "        # compute output and loss, without training\n",
    "        output = self(img, training=False)\n",
    "        loss = self.loss_function(label, output)\n",
    "\n",
    "        # update metrics\n",
    "        self.metrics_list[0][0].update_state(tf.argmax(output, axis=1), tf.argmax(label, axis=1))\n",
    "        self.metrics_list[0][1].update_state(loss)\n",
    "        self.metrics_list[0][2].update_state(self.compute_frobenius())\n",
    "\n",
    "        # return a dict with metric information \n",
    "        return {m.name : m.result() for m in self.metrics_list[0]}\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "id": "8svy39RCW_9o"
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_summary_writers(config_name):\n",
    "    \n",
    "    # Define where to save the logs\n",
    "    # along with this, you may want to save a config file with the same name so you know what the hyperparameters were used\n",
    "    # alternatively make a copy of the code that is used for later reference\n",
    "    \n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "    train_log_path = f\"logs/{config_name}/{current_time}/train\"\n",
    "    val_log_path = f\"logs/{config_name}/{current_time}/val\"\n",
    "\n",
    "    # log writer for training metrics\n",
    "    train_summary_writer = tf.summary.create_file_writer(train_log_path)\n",
    "\n",
    "    # log writer for validation metrics\n",
    "    val_summary_writer = tf.summary.create_file_writer(val_log_path)\n",
    "    \n",
    "    return train_summary_writer, val_summary_writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "id": "0R4loQRQW_9p"
   },
   "outputs": [],
   "source": [
    "# gets in a dataset and returns target values\n",
    "def prepare_data(dataset, seq_size, batch_size):\n",
    "\n",
    "    # convert data from uint8 to float32\n",
    "    dataset = dataset.map(lambda img, target: (tf.cast(img, tf.float32), target))\n",
    "    # convert image values from range [0, 255] to [-1, 1]\n",
    "    dataset = dataset.map(lambda img, target: ((img/128.)-1., target))\n",
    "\n",
    "    # Create random tuples of 4 images [[img, ...], [target, ...]]\n",
    "    dataset = dataset.batch(seq_size, drop_remainder=True)\n",
    "\n",
    "    #for a, b in dataset:\n",
    "      #print(list(b.as_numpy_iterator()))\n",
    "\n",
    "    for imgs, targets in dataset.take(1):\n",
    "      print(targets)\n",
    "      for target in targets:\n",
    "        print(target)\n",
    "\n",
    "    # Generate cumulative sum targets\n",
    "    def generate_sums(elems):\n",
    "      signs = tf.constant([1 if i % 2 == 0 else -1 for i in range(len(elems))], dtype=tf.int64)\n",
    "      return tf.cumsum(tf.math.multiply(elems, signs))\n",
    "\n",
    "    dataset = dataset.map(lambda imgs, targets: (imgs, generate_sums(targets)))\n",
    "\n",
    "    # Convert targets to one hot vectors\n",
    "    one_hot_depth = (int) (9*seq_size+1)\n",
    "    dataset = dataset.map(lambda imgs, targets: (imgs, tf.one_hot(targets + (int) (one_hot_depth / 2 + 1), depth=one_hot_depth)))\n",
    "\n",
    "    for imgs, targets in dataset.take(1):\n",
    "      print(targets)\n",
    "      for target in targets:\n",
    "        print(target)\n",
    "    \n",
    "    dataset = dataset.cache()\n",
    "    dataset = dataset.shuffle(4096)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "id": "GbGZjolXZZGH"
   },
   "outputs": [],
   "source": [
    "# trains the model by iterating through the dataset and applying training_step method epochs time\n",
    "def training_loop(model, train_ds, test_ds, epochs, train_summary_writer, memory):\n",
    "    metrics = []\n",
    "\n",
    "    # iterate over epochs\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "\n",
    "        # train steps on all batches in the training data\n",
    "        for (img, label) in train_ds:\n",
    "            metrics = model.train_step((img, label))\n",
    "            \n",
    "            # keep data in summary with metrics\n",
    "            with train_summary_writer.as_default():\n",
    "                for metric in model.metrics_list[1]:\n",
    "                    tf.summary.scalar(f\"{metric.name}\", metric.result(), step=epoch)\n",
    "\n",
    "        for (key, value) in metrics.items():\n",
    "            memory[key].append(value.numpy())\n",
    "\n",
    "        memory = test_loop(model=model,\n",
    "                           test_ds=test_ds,\n",
    "                           val_summary_writer=val_summary_writer,\n",
    "                           memory=memory)\n",
    "        \n",
    "        # print current metric values and reset the metrics\n",
    "        tf.print([f\"{key} : {value.numpy()}\" for (key, value ) in metrics.items()])\n",
    "        model.reset_metrics(1)\n",
    "\n",
    "    return memory\n",
    "\n",
    "\n",
    "# tests overall performance of model\n",
    "def test_loop(model, test_ds, val_summary_writer, memory):\n",
    "    metrics = []\n",
    "    # test steps on every item in test dataset\n",
    "    for (img, label) in tqdm(test_ds):\n",
    "        metrics = model.test_step((img, label))\n",
    "        \n",
    "        # keep data with metrics\n",
    "        with val_summary_writer.as_default():\n",
    "            for metric in model.metrics_list[0]:\n",
    "                tf.summary.scalar(f\"{metric.name}\", metric.result(), step=1)\n",
    "\n",
    "    for (key, value) in metrics.items():\n",
    "        memory[key].append(value.numpy())\n",
    "\n",
    "    print([f\"{key} : {value.numpy()}\" for (key, value ) in metrics.items()])\n",
    "\n",
    "    model.reset_metrics(0)\n",
    "\n",
    "    return memory\n",
    "\n",
    "# visualize accuracy, loss and frobenius norm\n",
    "def visualization(accuracies, losses, frobenius, name):\n",
    "    plt.figure()\n",
    "    line1, = plt.plot(accuracies, \"b\")\n",
    "    line2, = plt.plot(losses, \"r\")\n",
    "\n",
    "    frob_new = frobenius/np.max(frobenius) * np.max(losses)\n",
    "    line3, = plt.plot(frob_new, \"y\" )\n",
    "\n",
    "    plt.xlabel(\"Training steps\")\n",
    "    plt.ylabel(\"Loss/Accuracy\")\n",
    "    plt.legend((line1, line2, line3),(\"Accuracy\", \"Loss\", \"Frobenius Norm\"))\n",
    "    plt.savefig(name)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "id": "dDCxrRjh_WI8"
   },
   "outputs": [],
   "source": [
    "# overall hyperparameters to compare with and without overfitting precautions methods\n",
    "epochs = 15\n",
    "batch_size = 32\n",
    "seq_size = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dvQPXaqlBYxF"
   },
   "source": [
    "## no augmentation, normal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "id": "gziNcUqxD-M3",
    "outputId": "6f8c4c44-ffdb-41aa-a609-d7011a6bca13",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor([4 1 0 7], shape=(4,), dtype=int64)\n",
      "tf.Tensor(4, shape=(), dtype=int64)\n",
      "tf.Tensor(1, shape=(), dtype=int64)\n",
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(7, shape=(), dtype=int64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]], shape=(4, 37), dtype=float32)\n",
      "tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(37,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(37,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(37,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(37,), dtype=float32)\n",
      "tf.Tensor([2 0 4 8], shape=(4,), dtype=int64)\n",
      "tf.Tensor(2, shape=(), dtype=int64)\n",
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(4, shape=(), dtype=int64)\n",
      "tf.Tensor(8, shape=(), dtype=int64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]], shape=(4, 37), dtype=float32)\n",
      "tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(37,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(37,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(37,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(37,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "train_summary_writer, val_summary_writer = create_summary_writers(config_name=\"UNTOUCHED\")\n",
    "\n",
    "model = BasicConv(seq_size)\n",
    "\n",
    "\n",
    "#for img, label in train_ds.take(1):\n",
    "#    print(img.shape)\n",
    "#    print(label.shape)\n",
    "\n",
    "train_dataset = prepare_data(train_ds, seq_size, batch_size)\n",
    "#train_dataset = train_ds.apply(lambda dataset: prepare_data(dataset, seq_size, batch_size))\n",
    "val_dataset = prepare_data(val_ds, seq_size, batch_size)\n",
    "#val_dataset = val_ds.apply(lambda dataset: prepare_data(dataset, seq_size, batch_size))\n",
    "\n",
    "#for img, label in train_dataset.take(1):\n",
    "#    print(img.shape)\n",
    "#    print(label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "id": "8G0PkfIFZZGM",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b3687e7e-6d67-4c3f-ae76-42df7b5f815c"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/50\n",
      "469/469 [==============================] - 12s 20ms/step - loss: 2.9623 - accuracy: 0.0910 - val_loss: 2.8265 - val_accuracy: 0.1127\n",
      "Epoch 2/50\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 2.7402 - accuracy: 0.1295 - val_loss: 2.6514 - val_accuracy: 0.1470\n",
      "Epoch 3/50\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 2.5954 - accuracy: 0.1663 - val_loss: 2.4734 - val_accuracy: 0.2156\n",
      "Epoch 4/50\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 2.3255 - accuracy: 0.2538 - val_loss: 2.1495 - val_accuracy: 0.3071\n",
      "Epoch 5/50\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 2.0353 - accuracy: 0.3426 - val_loss: 1.9041 - val_accuracy: 0.3784\n",
      "Epoch 6/50\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 1.8182 - accuracy: 0.4077 - val_loss: 1.7329 - val_accuracy: 0.4285\n",
      "Epoch 7/50\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 1.6447 - accuracy: 0.4651 - val_loss: 1.5835 - val_accuracy: 0.4755\n",
      "Epoch 8/50\n",
      "469/469 [==============================] - 4s 10ms/step - loss: 1.5111 - accuracy: 0.5076 - val_loss: 1.4822 - val_accuracy: 0.5064\n",
      "Epoch 9/50\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 1.4115 - accuracy: 0.5383 - val_loss: 1.4006 - val_accuracy: 0.5383\n",
      "Epoch 10/50\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 1.3235 - accuracy: 0.5691 - val_loss: 1.2888 - val_accuracy: 0.5749\n",
      "Epoch 11/50\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 1.2414 - accuracy: 0.5986 - val_loss: 1.2639 - val_accuracy: 0.5962\n",
      "Epoch 12/50\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 1.1577 - accuracy: 0.6305 - val_loss: 1.1318 - val_accuracy: 0.6376\n",
      "Epoch 13/50\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 1.0691 - accuracy: 0.6626 - val_loss: 1.0469 - val_accuracy: 0.6721\n",
      "Epoch 14/50\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.9990 - accuracy: 0.6864 - val_loss: 1.0004 - val_accuracy: 0.6883\n",
      "Epoch 15/50\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.9339 - accuracy: 0.7079 - val_loss: 0.9327 - val_accuracy: 0.7068\n",
      "Epoch 16/50\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.8762 - accuracy: 0.7262 - val_loss: 0.8878 - val_accuracy: 0.7234\n",
      "Epoch 17/50\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.8287 - accuracy: 0.7436 - val_loss: 0.8483 - val_accuracy: 0.7357\n",
      "Epoch 18/50\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.7718 - accuracy: 0.7620 - val_loss: 0.8005 - val_accuracy: 0.7564\n",
      "Epoch 19/50\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.7248 - accuracy: 0.7768 - val_loss: 0.7551 - val_accuracy: 0.7640\n",
      "Epoch 20/50\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.6701 - accuracy: 0.7926 - val_loss: 0.7023 - val_accuracy: 0.7867\n",
      "Epoch 21/50\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.6312 - accuracy: 0.8033 - val_loss: 0.6619 - val_accuracy: 0.7978\n",
      "Epoch 22/50\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.5999 - accuracy: 0.8138 - val_loss: 0.6659 - val_accuracy: 0.7952\n",
      "Epoch 23/50\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.5596 - accuracy: 0.8267 - val_loss: 0.6357 - val_accuracy: 0.8084\n",
      "Epoch 24/50\n",
      "469/469 [==============================] - 4s 10ms/step - loss: 0.5357 - accuracy: 0.8354 - val_loss: 0.6095 - val_accuracy: 0.8174\n",
      "Epoch 25/50\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.5107 - accuracy: 0.8450 - val_loss: 0.5846 - val_accuracy: 0.8272\n",
      "Epoch 26/50\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.4858 - accuracy: 0.8526 - val_loss: 0.5880 - val_accuracy: 0.8246\n",
      "Epoch 27/50\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.4638 - accuracy: 0.8584 - val_loss: 0.5310 - val_accuracy: 0.8456\n",
      "Epoch 28/50\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.4439 - accuracy: 0.8678 - val_loss: 0.5751 - val_accuracy: 0.8322\n",
      "Epoch 29/50\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.4248 - accuracy: 0.8741 - val_loss: 0.5097 - val_accuracy: 0.8550\n",
      "Epoch 30/50\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.3944 - accuracy: 0.8828 - val_loss: 0.4663 - val_accuracy: 0.8646\n",
      "Epoch 31/50\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.3769 - accuracy: 0.8888 - val_loss: 0.4661 - val_accuracy: 0.8675\n",
      "Epoch 32/50\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.3608 - accuracy: 0.8956 - val_loss: 0.4342 - val_accuracy: 0.8822\n",
      "Epoch 33/50\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.3397 - accuracy: 0.9021 - val_loss: 0.5120 - val_accuracy: 0.8597\n",
      "Epoch 34/50\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.3227 - accuracy: 0.9068 - val_loss: 0.4454 - val_accuracy: 0.8797\n",
      "Epoch 35/50\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.3108 - accuracy: 0.9101 - val_loss: 0.4214 - val_accuracy: 0.8857\n",
      "Epoch 36/50\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.2917 - accuracy: 0.9155 - val_loss: 0.3762 - val_accuracy: 0.8977\n",
      "Epoch 37/50\n",
      "469/469 [==============================] - 4s 10ms/step - loss: 0.2752 - accuracy: 0.9204 - val_loss: 0.3895 - val_accuracy: 0.8946\n",
      "Epoch 38/50\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.2661 - accuracy: 0.9236 - val_loss: 0.4379 - val_accuracy: 0.8817\n",
      "Epoch 39/50\n",
      "469/469 [==============================] - 4s 10ms/step - loss: 0.2560 - accuracy: 0.9272 - val_loss: 0.3713 - val_accuracy: 0.8997\n",
      "Epoch 40/50\n",
      "469/469 [==============================] - 4s 10ms/step - loss: 0.2426 - accuracy: 0.9308 - val_loss: 0.3631 - val_accuracy: 0.9056\n",
      "Epoch 41/50\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.2324 - accuracy: 0.9342 - val_loss: 0.3629 - val_accuracy: 0.9016\n",
      "Epoch 42/50\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.2199 - accuracy: 0.9381 - val_loss: 0.3430 - val_accuracy: 0.9103\n",
      "Epoch 43/50\n",
      "469/469 [==============================] - 4s 10ms/step - loss: 0.2074 - accuracy: 0.9418 - val_loss: 0.3434 - val_accuracy: 0.9138\n",
      "Epoch 44/50\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.2032 - accuracy: 0.9423 - val_loss: 0.3496 - val_accuracy: 0.9120\n",
      "Epoch 45/50\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.1947 - accuracy: 0.9449 - val_loss: 0.3429 - val_accuracy: 0.9099\n",
      "Epoch 46/50\n",
      "469/469 [==============================] - 4s 10ms/step - loss: 0.1827 - accuracy: 0.9489 - val_loss: 0.3206 - val_accuracy: 0.9187\n",
      "Epoch 47/50\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.1789 - accuracy: 0.9504 - val_loss: 0.3248 - val_accuracy: 0.9148\n",
      "Epoch 48/50\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.1695 - accuracy: 0.9525 - val_loss: 0.3369 - val_accuracy: 0.9134\n",
      "Epoch 49/50\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.1556 - accuracy: 0.9568 - val_loss: 0.3046 - val_accuracy: 0.9208\n",
      "Epoch 50/50\n",
      "469/469 [==============================] - 4s 10ms/step - loss: 0.1517 - accuracy: 0.9577 - val_loss: 0.3050 - val_accuracy: 0.9221\n",
      "Model: \"basic_conv_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " max_pooling2d_32 (MaxPoolin  multiple                 0 (unused)\n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_64 (Conv2D)          multiple                  160       \n",
      "                                                                 \n",
      " conv2d_65 (Conv2D)          multiple                  2320      \n",
      "                                                                 \n",
      " time_distributed_50 (TimeDi  multiple                 0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " conv2d_66 (Conv2D)          multiple                  4640      \n",
      "                                                                 \n",
      " conv2d_67 (Conv2D)          multiple                  9248      \n",
      "                                                                 \n",
      " time_distributed_51 (TimeDi  multiple                 0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " rnn_2 (RNN)                 multiple                  24832     \n",
      "                                                                 \n",
      " time_distributed_52 (TimeDi  multiple                 2405      \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 43,605\n",
      "Trainable params: 43,605\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Test compile\n",
    "#print(train_dataset)\n",
    "#for imgs, targets in train_dataset.take(1):\n",
    "#  print(targets)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(), \n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              #loss=\"mean_squared_error\",\n",
    "              metrics=[\n",
    "                      tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
    "                      #tf.keras.metrics.Accuracy(name=\"accuracy\"),\n",
    "                      #tf.keras.metrics.Mean(name=\"frob_norm\")\n",
    "                      ]\n",
    "              )\n",
    "\n",
    "history = model.fit(train_dataset,\n",
    "                    validation_data=val_dataset,\n",
    "                    epochs=50)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "id": "Yd8MuzB_-7mb",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "outputId": "036289d3-9171-481b-b53e-cc9173e64d91"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\ntfds.benchmark(train_dataset, batch_size=batch_size)\\n\\nprint(\"\\n************ Training UNTOUCHED ************\\n\")\\n\\nmemory = {\"test_accuracy\" : [],\\n          \"test_loss\" : [],\\n          \"test_frob_norm\" : [],\\n          \"train_accuracy\" : [],\\n          \"train_loss\" : [],\\n          \"train_frob_norm\" : []\\n          }\\n\\nmemory = training_loop(model,\\n                       train_ds=train_dataset,\\n                       test_ds=val_dataset,\\n                       epochs=tf.constant(epochs),\\n                       train_summary_writer=train_summary_writer,\\n                       memory=memory)\\n\\nvisualization(memory[\"train_accuracy\"], memory[\"train_loss\"], memory[\"train_frob_norm\"], \"aug_train\")\\n'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 174
    }
   ],
   "source": [
    "\"\"\"\n",
    "tfds.benchmark(train_dataset, batch_size=batch_size)\n",
    "\n",
    "print(\"\\n************ Training UNTOUCHED ************\\n\")\n",
    "\n",
    "memory = {\"test_accuracy\" : [],\n",
    "          \"test_loss\" : [],\n",
    "          \"test_frob_norm\" : [],\n",
    "          \"train_accuracy\" : [],\n",
    "          \"train_loss\" : [],\n",
    "          \"train_frob_norm\" : []\n",
    "          }\n",
    "\n",
    "memory = training_loop(model,\n",
    "                       train_ds=train_dataset,\n",
    "                       test_ds=val_dataset,\n",
    "                       epochs=tf.constant(epochs),\n",
    "                       train_summary_writer=train_summary_writer,\n",
    "                       memory=memory)\n",
    "\n",
    "visualization(memory[\"train_accuracy\"], memory[\"train_loss\"], memory[\"train_frob_norm\"], \"aug_train\")\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "ki_uebungen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f29be5cdbd889ccae82f59fdf4d8dd89ee7979a66140a153cd0c4d342f0eb10a"
   }
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
