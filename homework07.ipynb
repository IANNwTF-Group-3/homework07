{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "id": "U1lUFaCWW_9l",
    "outputId": "d06e93a1-a312-422a-8898-48bc0e707546",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Num GPUs Available:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# disable compiler warnings\n",
    "import os\n",
    "\n",
    "# imports \n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "np_config.enable_numpy_behavior()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from typing import List\n",
    "import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'  # FATAL\n",
    "print(\"Num GPUs Available: \", tf.config.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "id": "uk6WUyODW_9n"
   },
   "outputs": [],
   "source": [
    "(train_ds, val_ds), ds_info = tfds.load('mnist', split=['train', 'test'], as_supervised=True, with_info=True)\n",
    "\n",
    "#tfds.show_examples(train_ds, ds_info)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "class CustomLSTM(tf.keras.layers.AbstractRNNCell):\n",
    "  def __init__(self, units, **kwargs):\n",
    "      self.units = units\n",
    "      super(CustomLSTM, self).__init__(**kwargs)\n",
    "\n",
    "      initializer = tf.keras.initializers.Orthogonal()\n",
    "\n",
    "      self.layer_information_eraser = tf.keras.layers.Dense(self.units, activation='sigmoid', kernel_initializer=initializer)\n",
    "      self.layer_new_information_filter = tf.keras.layers.Dense(self.units, activation='sigmoid', kernel_initializer=initializer)\n",
    "      self.layer_new_information = tf.keras.layers.Dense(self.units, activation='tanh', kernel_initializer=initializer)\n",
    "      self.layer_information_transfer_filter = tf.keras.layers.Dense(self.units, activation='sigmoid', kernel_initializer=initializer)\n",
    "\n",
    "  @property\n",
    "  def state_size(self):\n",
    "    return (self.units, self.units)\n",
    "    #return [tf.TensorShape([self.units]), tf.TensorShape([self.units])]\n",
    "\n",
    "  @property\n",
    "  def output_size(self):\n",
    "    return self.units\n",
    "\n",
    "  #def get_initial_state(self, inputs=None, batch_size=None, dtype=None):\n",
    "    #return [tf.zeros([self.units], tf.float32), tf.zeros([self.units], tf.float32)]\n",
    "\n",
    "  def call(self, inputs, states):\n",
    "    hidden_state, cell_state = states\n",
    "\n",
    "    hidden_input = tf.concat([inputs, hidden_state], 1)\n",
    "\n",
    "    cell_state = tf.math.multiply(cell_state, self.layer_information_eraser(hidden_input))\n",
    "    cell_state = tf.math.add(cell_state, tf.math.multiply(self.layer_new_information(hidden_input), self.layer_new_information_filter(hidden_input)))\n",
    "\n",
    "    hidden_state = tf.math.multiply(tf.math.tanh(cell_state), self.layer_information_transfer_filter(hidden_input))\n",
    "\n",
    "    return hidden_state, [hidden_state, cell_state]"
   ],
   "metadata": {
    "id": "DEFe60MFOII0"
   },
   "execution_count": 155,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "id": "jVBPWYKzW_9n"
   },
   "outputs": [],
   "source": [
    "class BasicConv(tf.keras.Model):\n",
    "    def __init__(self, seq_size, optimizer=tf.keras.optimizers.Adam()):\n",
    "        super(BasicConv, self).__init__()\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "        #self.loss_function = tf.keras.losses.CategoricalCrossentropy()\n",
    "        #self.metrics_list = [[tf.keras.metrics.CategoricalAccuracy(name=\"test_accuracy\"),\n",
    "        #                     tf.keras.metrics.Mean(name=\"test_loss\"),\n",
    "        #                     tf.keras.metrics.Mean(name=\"test_frob_norm\")],\n",
    "        #                     [tf.keras.metrics.CategoricalAccuracy(name=\"train_accuracy\"),\n",
    "        #                     tf.keras.metrics.Mean(name=\"train_loss\"),\n",
    "        #                     tf.keras.metrics.Mean(name=\"train_frob_norm\")]]\n",
    "\n",
    "        #self.metrics_list = [tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
    "        #                     tf.keras.metrics.Mean(name=\"loss\"),\n",
    "        #                     tf.keras.metrics.Mean(name=\"frob_norm\")]\n",
    "\n",
    "        output_size = (int) (9*seq_size+1)\n",
    "  \n",
    "        self.pooling = tf.keras.layers.MaxPooling2D()\n",
    "        self.my_layers = [\n",
    "                        tf.keras.layers.Conv2D(filters=16, kernel_size=3, padding='same', activation='relu', input_shape=(28, 28, 1)),\n",
    "                        tf.keras.layers.Conv2D(filters=16, kernel_size=3, padding='same', activation='relu', input_shape=(28, 28, 1)),\n",
    "                        tf.keras.layers.TimeDistributed(tf.keras.layers.MaxPooling2D()),\n",
    "                        tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding='same', activation='relu', input_shape=(14, 14, 1)),\n",
    "                        tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding='same', activation='relu', input_shape=(14, 14, 1)),\n",
    "                        tf.keras.layers.TimeDistributed(tf.keras.layers.GlobalAvgPool2D()),\n",
    "                        #tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(10, activation='softmax')),\n",
    "                        #tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(1, activation='relu')),\n",
    "                        tf.keras.layers.RNN(CustomLSTM(32), unroll=True, return_sequences=True),\n",
    "                        #tf.keras.layers.LSTM(20, unroll=True, return_sequences=True),\n",
    "                        #tf.keras.layers.Dense(output_size, activation='softmax')\n",
    "                        tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(output_size, activation='softmax'))\n",
    "                        ]\n",
    "\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, x, training=False):\n",
    "        x = self.my_layers[0](x)\n",
    "        x = self.my_layers[1](x)\n",
    "        x = self.my_layers[2](x)\n",
    "        x = self.my_layers[3](x)\n",
    "        x = self.my_layers[4](x)\n",
    "        x = self.my_layers[5](x)\n",
    "        x = self.my_layers[6](x)\n",
    "        x = self.my_layers[7](x)\n",
    "        #x = self.my_layers[8](x)\n",
    "        #x = tf.round(x)\n",
    "        \n",
    "        #for layer in self.my_layers:\n",
    "        #    tf.print(x)\n",
    "        #    x = layer(x)\n",
    "        return x\n",
    "\n",
    "    @tf.function\n",
    "    def compute_frobenius(self):\n",
    "        frobenius_norm = tf.zeros((1,))\n",
    "        for var in self.trainable_variables:\n",
    "            frobenius_norm += tf.norm(var, ord=\"euclidean\")\n",
    "        return frobenius_norm\n",
    "\n",
    "    # 3. metrics property\n",
    "    #\"\"\"@property\"\"\"\n",
    "    #def metrics(self):\n",
    "    #    return self.metrics_list\n",
    "\n",
    "    # 4. reset all metrics objects\n",
    "    #def reset_metrics(self):\n",
    "    #    for metric in self.metrics:\n",
    "    #      #for metric in metric_list:\n",
    "    #      metric.reset_states()\n",
    "\n",
    "    \"\"\"\n",
    "    # train_step method\n",
    "    @tf.function\n",
    "    def train_step(self, data):\n",
    "        img, label = data\n",
    "        \n",
    "        # compute output and loss, train the variables\n",
    "        with tf.GradientTape() as tape:\n",
    "            output = self(img, training=True)\n",
    "            loss = self.loss_function(label, output)\n",
    "            \n",
    "        # update trainable variables\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "\n",
    "        # update metrics\n",
    "        self.metrics_list[1][0].update_state(tf.argmax(output, axis=1), tf.argmax(label, axis=1))\n",
    "        self.metrics_list[1][1].update_state(loss)\n",
    "        self.metrics_list[1][2].update_state(self.compute_frobenius())\n",
    "        \n",
    "        # return a dict with metric information\n",
    "        return {m.name : m.result() for m in self.metrics_list[1]}\n",
    "\n",
    "\n",
    "\n",
    "    # test_step method\n",
    "    @tf.function\n",
    "    def test_step(self, data):\n",
    "        img, label = data\n",
    "\n",
    "        # compute output and loss, without training\n",
    "        output = self(img, training=False)\n",
    "        loss = self.loss_function(label, output)\n",
    "\n",
    "        # update metrics\n",
    "        self.metrics_list[0][0].update_state(tf.argmax(output, axis=1), tf.argmax(label, axis=1))\n",
    "        self.metrics_list[0][1].update_state(loss)\n",
    "        self.metrics_list[0][2].update_state(self.compute_frobenius())\n",
    "\n",
    "        # return a dict with metric information \n",
    "        return {m.name : m.result() for m in self.metrics_list[0]}\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "id": "8svy39RCW_9o"
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_summary_writers(config_name):\n",
    "    \n",
    "    # Define where to save the logs\n",
    "    # along with this, you may want to save a config file with the same name so you know what the hyperparameters were used\n",
    "    # alternatively make a copy of the code that is used for later reference\n",
    "    \n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "    train_log_path = f\"logs/{config_name}/{current_time}/train\"\n",
    "    val_log_path = f\"logs/{config_name}/{current_time}/val\"\n",
    "\n",
    "    # log writer for training metrics\n",
    "    train_summary_writer = tf.summary.create_file_writer(train_log_path)\n",
    "\n",
    "    # log writer for validation metrics\n",
    "    val_summary_writer = tf.summary.create_file_writer(val_log_path)\n",
    "    \n",
    "    return train_summary_writer, val_summary_writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "id": "0R4loQRQW_9p"
   },
   "outputs": [],
   "source": [
    "# gets in a dataset and returns target values\n",
    "def prepare_data(dataset, seq_size, batch_size):\n",
    "\n",
    "    # convert data from uint8 to float32\n",
    "    dataset = dataset.map(lambda img, target: (tf.cast(img, tf.float32), target))\n",
    "    # convert image values from range [0, 255] to [-1, 1]\n",
    "    dataset = dataset.map(lambda img, target: ((img/128.)-1., target))\n",
    "\n",
    "    # Create random tuples of 4 images [[img, ...], [target, ...]]\n",
    "    dataset = dataset.batch(seq_size, drop_remainder=True)\n",
    "\n",
    "    #for a, b in dataset:\n",
    "      #print(list(b.as_numpy_iterator()))\n",
    "\n",
    "    for imgs, targets in dataset.take(1):\n",
    "      print(targets)\n",
    "      for target in targets:\n",
    "        print(target)\n",
    "\n",
    "    # Generate cumulative sum targets\n",
    "    def generate_sums(elems):\n",
    "      signs = tf.constant([1 if i % 2 == 0 else -1 for i in range(len(elems))], dtype=tf.int64)\n",
    "      return tf.cumsum(tf.math.multiply(elems, signs))\n",
    "\n",
    "    dataset = dataset.map(lambda imgs, targets: (imgs, generate_sums(targets)))\n",
    "\n",
    "    # Convert targets to one hot vectors\n",
    "    one_hot_depth = (int) (9*seq_size+1)\n",
    "    dataset = dataset.map(lambda imgs, targets: (imgs, tf.one_hot(targets + (int) (one_hot_depth / 2 + 1), depth=one_hot_depth)))\n",
    "\n",
    "    for imgs, targets in dataset.take(1):\n",
    "      print(targets)\n",
    "      for target in targets:\n",
    "        print(target)\n",
    "    \n",
    "    dataset = dataset.cache()\n",
    "    dataset = dataset.shuffle(4096)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "id": "GbGZjolXZZGH"
   },
   "outputs": [],
   "source": [
    "# trains the model by iterating through the dataset and applying training_step method epochs time\n",
    "def training_loop(model, train_ds, test_ds, epochs, train_summary_writer, memory):\n",
    "    metrics = []\n",
    "\n",
    "    # iterate over epochs\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "\n",
    "        # train steps on all batches in the training data\n",
    "        for (img, label) in train_ds:\n",
    "            metrics = model.train_step((img, label))\n",
    "            \n",
    "            # keep data in summary with metrics\n",
    "            with train_summary_writer.as_default():\n",
    "                for metric in model.metrics_list[1]:\n",
    "                    tf.summary.scalar(f\"{metric.name}\", metric.result(), step=epoch)\n",
    "\n",
    "        for (key, value) in metrics.items():\n",
    "            memory[key].append(value.numpy())\n",
    "\n",
    "        memory = test_loop(model=model,\n",
    "                           test_ds=test_ds,\n",
    "                           val_summary_writer=val_summary_writer,\n",
    "                           memory=memory)\n",
    "        \n",
    "        # print current metric values and reset the metrics\n",
    "        tf.print([f\"{key} : {value.numpy()}\" for (key, value ) in metrics.items()])\n",
    "        model.reset_metrics(1)\n",
    "\n",
    "    return memory\n",
    "\n",
    "\n",
    "# tests overall performance of model\n",
    "def test_loop(model, test_ds, val_summary_writer, memory):\n",
    "    metrics = []\n",
    "    # test steps on every item in test dataset\n",
    "    for (img, label) in tqdm(test_ds):\n",
    "        metrics = model.test_step((img, label))\n",
    "        \n",
    "        # keep data with metrics\n",
    "        with val_summary_writer.as_default():\n",
    "            for metric in model.metrics_list[0]:\n",
    "                tf.summary.scalar(f\"{metric.name}\", metric.result(), step=1)\n",
    "\n",
    "    for (key, value) in metrics.items():\n",
    "        memory[key].append(value.numpy())\n",
    "\n",
    "    print([f\"{key} : {value.numpy()}\" for (key, value ) in metrics.items()])\n",
    "\n",
    "    model.reset_metrics(0)\n",
    "\n",
    "    return memory\n",
    "\n",
    "# visualize accuracy, loss and frobenius norm\n",
    "def visualization(accuracies, losses, frobenius, name):\n",
    "    plt.figure()\n",
    "    line1, = plt.plot(accuracies, \"b\")\n",
    "    line2, = plt.plot(losses, \"r\")\n",
    "\n",
    "    frob_new = frobenius/np.max(frobenius) * np.max(losses)\n",
    "    line3, = plt.plot(frob_new, \"y\" )\n",
    "\n",
    "    plt.xlabel(\"Training steps\")\n",
    "    plt.ylabel(\"Loss/Accuracy\")\n",
    "    plt.legend((line1, line2, line3),(\"Accuracy\", \"Loss\", \"Frobenius Norm\"))\n",
    "    plt.savefig(name)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "id": "dDCxrRjh_WI8"
   },
   "outputs": [],
   "source": [
    "# overall hyperparameters to compare with and without overfitting precautions methods\n",
    "epochs = 15\n",
    "batch_size = 32\n",
    "seq_size = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dvQPXaqlBYxF"
   },
   "source": [
    "## no augmentation, normal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "id": "gziNcUqxD-M3",
    "outputId": "eb905b76-d083-49de-86c2-f173cbfd80b6",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor([4 1 0 7], shape=(4,), dtype=int64)\n",
      "tf.Tensor(4, shape=(), dtype=int64)\n",
      "tf.Tensor(1, shape=(), dtype=int64)\n",
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(7, shape=(), dtype=int64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]], shape=(4, 37), dtype=float32)\n",
      "tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(37,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(37,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(37,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(37,), dtype=float32)\n",
      "tf.Tensor([2 0 4 8], shape=(4,), dtype=int64)\n",
      "tf.Tensor(2, shape=(), dtype=int64)\n",
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(4, shape=(), dtype=int64)\n",
      "tf.Tensor(8, shape=(), dtype=int64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]], shape=(4, 37), dtype=float32)\n",
      "tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(37,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(37,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(37,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(37,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "train_summary_writer, val_summary_writer = create_summary_writers(config_name=\"UNTOUCHED\")\n",
    "\n",
    "model = BasicConv(seq_size)\n",
    "\n",
    "\n",
    "#for img, label in train_ds.take(1):\n",
    "#    print(img.shape)\n",
    "#    print(label.shape)\n",
    "\n",
    "train_dataset = prepare_data(train_ds, seq_size, batch_size)\n",
    "#train_dataset = train_ds.apply(lambda dataset: prepare_data(dataset, seq_size, batch_size))\n",
    "val_dataset = prepare_data(val_ds, seq_size, batch_size)\n",
    "#val_dataset = val_ds.apply(lambda dataset: prepare_data(dataset, seq_size, batch_size))\n",
    "\n",
    "#for img, label in train_dataset.take(1):\n",
    "#    print(img.shape)\n",
    "#    print(label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "id": "8G0PkfIFZZGM",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "bb85fbf6-176d-4ce2-e92b-91b2225e8c5d"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/50\n",
      "469/469 [==============================] - 12s 20ms/step - loss: 3.0248 - accuracy: 0.0855 - val_loss: 2.8926 - val_accuracy: 0.1100\n",
      "Epoch 2/50\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 2.8113 - accuracy: 0.1222 - val_loss: 2.7304 - val_accuracy: 0.1414\n",
      "Epoch 3/50\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 2.6181 - accuracy: 0.1894 - val_loss: 2.4588 - val_accuracy: 0.2418\n",
      "Epoch 4/50\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 2.3165 - accuracy: 0.3036 - val_loss: 2.1633 - val_accuracy: 0.3480\n",
      "Epoch 5/50\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 2.0476 - accuracy: 0.3650 - val_loss: 1.9401 - val_accuracy: 0.3891\n",
      "Epoch 6/50\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 1.8408 - accuracy: 0.4124 - val_loss: 1.7490 - val_accuracy: 0.4402\n",
      "Epoch 7/50\n",
      "469/469 [==============================] - 4s 10ms/step - loss: 1.6877 - accuracy: 0.4575 - val_loss: 1.6370 - val_accuracy: 0.4693\n",
      "Epoch 8/50\n",
      "469/469 [==============================] - 4s 10ms/step - loss: 1.5746 - accuracy: 0.4897 - val_loss: 1.5278 - val_accuracy: 0.4999\n",
      "Epoch 9/50\n",
      "469/469 [==============================] - 4s 10ms/step - loss: 1.4836 - accuracy: 0.5186 - val_loss: 1.4553 - val_accuracy: 0.5255\n",
      "Epoch 10/50\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 1.4001 - accuracy: 0.5442 - val_loss: 1.3649 - val_accuracy: 0.5572\n",
      "Epoch 11/50\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 1.3065 - accuracy: 0.5781 - val_loss: 1.2550 - val_accuracy: 0.6031\n",
      "Epoch 12/50\n",
      "469/469 [==============================] - 4s 10ms/step - loss: 1.1842 - accuracy: 0.6216 - val_loss: 1.1357 - val_accuracy: 0.6420\n",
      "Epoch 13/50\n",
      "469/469 [==============================] - 4s 10ms/step - loss: 1.0823 - accuracy: 0.6538 - val_loss: 1.0675 - val_accuracy: 0.6590\n",
      "Epoch 14/50\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 1.0002 - accuracy: 0.6797 - val_loss: 0.9911 - val_accuracy: 0.6830\n",
      "Epoch 15/50\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.9409 - accuracy: 0.6971 - val_loss: 0.9120 - val_accuracy: 0.7088\n",
      "Epoch 16/50\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.8875 - accuracy: 0.7135 - val_loss: 0.8928 - val_accuracy: 0.7128\n",
      "Epoch 17/50\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.8438 - accuracy: 0.7259 - val_loss: 0.8638 - val_accuracy: 0.7235\n",
      "Epoch 18/50\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.8118 - accuracy: 0.7363 - val_loss: 0.8130 - val_accuracy: 0.7369\n",
      "Epoch 19/50\n",
      "469/469 [==============================] - 4s 10ms/step - loss: 0.7818 - accuracy: 0.7444 - val_loss: 0.7834 - val_accuracy: 0.7491\n",
      "Epoch 20/50\n",
      "469/469 [==============================] - 4s 10ms/step - loss: 0.7478 - accuracy: 0.7567 - val_loss: 0.7942 - val_accuracy: 0.7493\n",
      "Epoch 21/50\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.7214 - accuracy: 0.7652 - val_loss: 0.7305 - val_accuracy: 0.7616\n",
      "Epoch 22/50\n",
      "469/469 [==============================] - 4s 10ms/step - loss: 0.6941 - accuracy: 0.7740 - val_loss: 0.7324 - val_accuracy: 0.7598\n",
      "Epoch 23/50\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.6682 - accuracy: 0.7815 - val_loss: 0.7000 - val_accuracy: 0.7745\n",
      "Epoch 24/50\n",
      "469/469 [==============================] - 4s 10ms/step - loss: 0.6477 - accuracy: 0.7892 - val_loss: 0.6997 - val_accuracy: 0.7773\n",
      "Epoch 25/50\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.6312 - accuracy: 0.7947 - val_loss: 0.6851 - val_accuracy: 0.7769\n",
      "Epoch 26/50\n",
      "469/469 [==============================] - 4s 10ms/step - loss: 0.6043 - accuracy: 0.8040 - val_loss: 0.6635 - val_accuracy: 0.7914\n",
      "Epoch 27/50\n",
      "469/469 [==============================] - 4s 10ms/step - loss: 0.5891 - accuracy: 0.8095 - val_loss: 0.7206 - val_accuracy: 0.7748\n",
      "Epoch 28/50\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.5678 - accuracy: 0.8151 - val_loss: 0.6197 - val_accuracy: 0.8061\n",
      "Epoch 29/50\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.5539 - accuracy: 0.8211 - val_loss: 0.6432 - val_accuracy: 0.8013\n",
      "Epoch 30/50\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.5360 - accuracy: 0.8262 - val_loss: 0.5978 - val_accuracy: 0.8115\n",
      "Epoch 31/50\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.5206 - accuracy: 0.8331 - val_loss: 0.5929 - val_accuracy: 0.8131\n",
      "Epoch 32/50\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.5066 - accuracy: 0.8371 - val_loss: 0.5791 - val_accuracy: 0.8208\n",
      "Epoch 33/50\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.4850 - accuracy: 0.8453 - val_loss: 0.6062 - val_accuracy: 0.8121\n",
      "Epoch 34/50\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.4759 - accuracy: 0.8471 - val_loss: 0.5635 - val_accuracy: 0.8283\n",
      "Epoch 35/50\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.4587 - accuracy: 0.8537 - val_loss: 0.5416 - val_accuracy: 0.8299\n",
      "Epoch 36/50\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.4491 - accuracy: 0.8572 - val_loss: 0.5138 - val_accuracy: 0.8427\n",
      "Epoch 37/50\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.4263 - accuracy: 0.8645 - val_loss: 0.5223 - val_accuracy: 0.8424\n",
      "Epoch 38/50\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.4242 - accuracy: 0.8653 - val_loss: 0.5078 - val_accuracy: 0.8432\n",
      "Epoch 39/50\n",
      "469/469 [==============================] - 4s 10ms/step - loss: 0.4060 - accuracy: 0.8706 - val_loss: 0.5031 - val_accuracy: 0.8509\n",
      "Epoch 40/50\n",
      "469/469 [==============================] - 4s 10ms/step - loss: 0.4014 - accuracy: 0.8729 - val_loss: 0.4936 - val_accuracy: 0.8519\n",
      "Epoch 41/50\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3880 - accuracy: 0.8784 - val_loss: 0.4905 - val_accuracy: 0.8526\n",
      "Epoch 42/50\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.3741 - accuracy: 0.8824 - val_loss: 0.4799 - val_accuracy: 0.8534\n",
      "Epoch 43/50\n",
      "469/469 [==============================] - 4s 10ms/step - loss: 0.3644 - accuracy: 0.8865 - val_loss: 0.4765 - val_accuracy: 0.8607\n",
      "Epoch 44/50\n",
      "469/469 [==============================] - 4s 10ms/step - loss: 0.3620 - accuracy: 0.8872 - val_loss: 0.4851 - val_accuracy: 0.8509\n",
      "Epoch 45/50\n",
      "469/469 [==============================] - 4s 10ms/step - loss: 0.3470 - accuracy: 0.8918 - val_loss: 0.4699 - val_accuracy: 0.8604\n",
      "Epoch 46/50\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3381 - accuracy: 0.8942 - val_loss: 0.4449 - val_accuracy: 0.8702\n",
      "Epoch 47/50\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3215 - accuracy: 0.8999 - val_loss: 0.4626 - val_accuracy: 0.8693\n",
      "Epoch 48/50\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.3178 - accuracy: 0.9007 - val_loss: 0.4478 - val_accuracy: 0.8718\n",
      "Epoch 49/50\n",
      "469/469 [==============================] - 4s 10ms/step - loss: 0.3101 - accuracy: 0.9036 - val_loss: 0.4483 - val_accuracy: 0.8731\n",
      "Epoch 50/50\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.3028 - accuracy: 0.9063 - val_loss: 0.4304 - val_accuracy: 0.8810\n",
      "Model: \"basic_conv_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " max_pooling2d_30 (MaxPoolin  multiple                 0 (unused)\n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_60 (Conv2D)          multiple                  160       \n",
      "                                                                 \n",
      " conv2d_61 (Conv2D)          multiple                  2320      \n",
      "                                                                 \n",
      " time_distributed_47 (TimeDi  multiple                 0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " conv2d_62 (Conv2D)          multiple                  4640      \n",
      "                                                                 \n",
      " conv2d_63 (Conv2D)          multiple                  9248      \n",
      "                                                                 \n",
      " time_distributed_48 (TimeDi  multiple                 0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " rnn_1 (RNN)                 multiple                  8320      \n",
      "                                                                 \n",
      " time_distributed_49 (TimeDi  multiple                 1221      \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25,909\n",
      "Trainable params: 25,909\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Test compile\n",
    "#print(train_dataset)\n",
    "#for imgs, targets in train_dataset.take(1):\n",
    "#  print(targets)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(), \n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              #loss=\"mean_squared_error\",\n",
    "              metrics=[\n",
    "                      tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
    "                      #tf.keras.metrics.Accuracy(name=\"accuracy\"),\n",
    "                      #tf.keras.metrics.Mean(name=\"frob_norm\")\n",
    "                      ]\n",
    "              )\n",
    "\n",
    "history = model.fit(train_dataset,\n",
    "                    validation_data=val_dataset,\n",
    "                    epochs=50)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "id": "Yd8MuzB_-7mb",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "outputId": "74e9f2b3-24c4-432a-d969-ed000d7737aa"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\ntfds.benchmark(train_dataset, batch_size=batch_size)\\n\\nprint(\"\\n************ Training UNTOUCHED ************\\n\")\\n\\nmemory = {\"test_accuracy\" : [],\\n          \"test_loss\" : [],\\n          \"test_frob_norm\" : [],\\n          \"train_accuracy\" : [],\\n          \"train_loss\" : [],\\n          \"train_frob_norm\" : []\\n          }\\n\\nmemory = training_loop(model,\\n                       train_ds=train_dataset,\\n                       test_ds=val_dataset,\\n                       epochs=tf.constant(epochs),\\n                       train_summary_writer=train_summary_writer,\\n                       memory=memory)\\n\\nvisualization(memory[\"train_accuracy\"], memory[\"train_loss\"], memory[\"train_frob_norm\"], \"aug_train\")\\n'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 163
    }
   ],
   "source": [
    "\"\"\"\n",
    "tfds.benchmark(train_dataset, batch_size=batch_size)\n",
    "\n",
    "print(\"\\n************ Training UNTOUCHED ************\\n\")\n",
    "\n",
    "memory = {\"test_accuracy\" : [],\n",
    "          \"test_loss\" : [],\n",
    "          \"test_frob_norm\" : [],\n",
    "          \"train_accuracy\" : [],\n",
    "          \"train_loss\" : [],\n",
    "          \"train_frob_norm\" : []\n",
    "          }\n",
    "\n",
    "memory = training_loop(model,\n",
    "                       train_ds=train_dataset,\n",
    "                       test_ds=val_dataset,\n",
    "                       epochs=tf.constant(epochs),\n",
    "                       train_summary_writer=train_summary_writer,\n",
    "                       memory=memory)\n",
    "\n",
    "visualization(memory[\"train_accuracy\"], memory[\"train_loss\"], memory[\"train_frob_norm\"], \"aug_train\")\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "ki_uebungen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f29be5cdbd889ccae82f59fdf4d8dd89ee7979a66140a153cd0c4d342f0eb10a"
   }
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
