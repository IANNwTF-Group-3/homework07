{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "id": "U1lUFaCWW_9l",
    "outputId": "856f81e1-d217-4edd-bd85-9ff4ff896c5a",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Num GPUs Available:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# disable compiler warnings\n",
    "import os\n",
    "\n",
    "# imports \n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "np_config.enable_numpy_behavior()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from typing import List\n",
    "import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'  # FATAL\n",
    "print(\"Num GPUs Available: \", tf.config.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "id": "uk6WUyODW_9n"
   },
   "outputs": [],
   "source": [
    "(train_ds, val_ds), ds_info = tfds.load('mnist', split=['train', 'test'], as_supervised=True, with_info=True)\n",
    "\n",
    "#tfds.show_examples(train_ds, ds_info)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "class CustomLSTM(tf.keras.layers.AbstractRNNCell):\n",
    "  def __init__(self, units, **kwargs):\n",
    "      self.units = units\n",
    "      super(CustomLSTM, self).__init__(**kwargs)\n",
    "\n",
    "      self.layer_information_eraser = tf.keras.layers.Dense(self.units, activation='sigmoid')\n",
    "      self.layer_new_information_filter = tf.keras.layers.Dense(self.units, activation='sigmoid')\n",
    "      self.layer_new_information = tf.keras.layers.Dense(self.units, activation='tanh')\n",
    "      self.layer_information_transfer_filter = tf.keras.layers.Dense(self.units, activation='sigmoid')\n",
    "\n",
    "  @property\n",
    "  def state_size(self):\n",
    "    return (self.units, self.units)\n",
    "    #return [tf.TensorShape([self.units]), tf.TensorShape([self.units])]\n",
    "\n",
    "  @property\n",
    "  def output_size(self):\n",
    "    return self.units\n",
    "\n",
    "  #def get_initial_state(self, inputs=None, batch_size=None, dtype=None):\n",
    "    #return [tf.zeros([self.units], tf.float32), tf.zeros([self.units], tf.float32)]\n",
    "\n",
    "  def call(self, inputs, states):\n",
    "    hidden_state, cell_state = states\n",
    "\n",
    "    hidden_input = tf.concat([inputs, hidden_state], 1)\n",
    "\n",
    "    cell_state = tf.math.multiply(cell_state, self.layer_information_eraser(hidden_input))\n",
    "    cell_state = tf.math.add(cell_state, tf.math.multiply(self.layer_new_information(hidden_input), self.layer_new_information_filter(hidden_input)))\n",
    "\n",
    "    hidden_state = tf.math.multiply(tf.math.tanh(cell_state), self.layer_information_transfer_filter(hidden_input))\n",
    "    \n",
    "    return hidden_state, [hidden_state, cell_state]"
   ],
   "metadata": {
    "id": "DEFe60MFOII0"
   },
   "execution_count": 167,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "id": "jVBPWYKzW_9n"
   },
   "outputs": [],
   "source": [
    "class BasicConv(tf.keras.Model):\n",
    "    def __init__(self, seq_size, optimizer=tf.keras.optimizers.Adam()):\n",
    "        super(BasicConv, self).__init__()\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_function = tf.keras.losses.CategoricalCrossentropy()\n",
    "        #self.metrics_list = [[tf.keras.metrics.CategoricalAccuracy(name=\"test_accuracy\"),\n",
    "        #                     tf.keras.metrics.Mean(name=\"test_loss\"),\n",
    "        #                     tf.keras.metrics.Mean(name=\"test_frob_norm\")],\n",
    "        #                     [tf.keras.metrics.CategoricalAccuracy(name=\"train_accuracy\"),\n",
    "        #                     tf.keras.metrics.Mean(name=\"train_loss\"),\n",
    "        #                     tf.keras.metrics.Mean(name=\"train_frob_norm\")]]\n",
    "\n",
    "        #self.metrics_list = [tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
    "        #                     tf.keras.metrics.Mean(name=\"loss\"),\n",
    "        #                     tf.keras.metrics.Mean(name=\"frob_norm\")]\n",
    "\n",
    "        output_size = (int) (9*(seq_size/2))\n",
    "  \n",
    "        self.pooling = tf.keras.layers.MaxPooling2D()\n",
    "        self.my_layers = [\n",
    "                        tf.keras.layers.Conv2D(filters=16, kernel_size=3, padding='same', activation='relu', input_shape=(28, 28, 1)),\n",
    "                        tf.keras.layers.Conv2D(filters=16, kernel_size=3, padding='same', activation='relu', input_shape=(28, 28, 1)),\n",
    "                        tf.keras.layers.TimeDistributed(tf.keras.layers.MaxPooling2D()),\n",
    "                        tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding='same', activation='relu', input_shape=(14, 14, 1)),\n",
    "                        tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding='same', activation='relu', input_shape=(14, 14, 1)),\n",
    "                        tf.keras.layers.TimeDistributed(tf.keras.layers.GlobalAvgPool2D()),\n",
    "                        tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(10, activation='softmax')),\n",
    "                        tf.keras.layers.RNN(CustomLSTM(8), unroll=True, return_sequences=True),\n",
    "                        #tf.keras.layers.LSTM(8, unroll=True, return_sequences=True),\n",
    "                        tf.keras.layers.Dense(output_size, activation='softmax')\n",
    "                        ]\n",
    "\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, x, training=False):\n",
    "        x = self.my_layers[0](x)\n",
    "        x = self.my_layers[1](x)\n",
    "        x = self.my_layers[2](x)\n",
    "        x = self.my_layers[3](x)\n",
    "        x = self.my_layers[4](x)\n",
    "        x = self.my_layers[5](x)\n",
    "        x = self.my_layers[6](x)\n",
    "        x = self.my_layers[7](x)\n",
    "        x = self.my_layers[8](x)\n",
    "        \n",
    "        #for layer in self.my_layers:\n",
    "        #    tf.print(x)\n",
    "        #    x = layer(x)\n",
    "        return x\n",
    "\n",
    "    @tf.function\n",
    "    def compute_frobenius(self):\n",
    "        frobenius_norm = tf.zeros((1,))\n",
    "        for var in self.trainable_variables:\n",
    "            frobenius_norm += tf.norm(var, ord=\"euclidean\")\n",
    "        return frobenius_norm\n",
    "\n",
    "    # 3. metrics property\n",
    "    #@property\n",
    "    #def metrics(self):\n",
    "    #    return self.metrics_list\n",
    "\n",
    "    # 4. reset all metrics objects\n",
    "    #def reset_metrics(self):\n",
    "    #    for metric in self.metrics:\n",
    "    #      #for metric in metric_list:\n",
    "    #      metric.reset_states()\n",
    "\n",
    "    \"\"\"\n",
    "    # train_step method\n",
    "    @tf.function\n",
    "    def train_step(self, data):\n",
    "        img, label = data\n",
    "        \n",
    "        # compute output and loss, train the variables\n",
    "        with tf.GradientTape() as tape:\n",
    "            output = self(img, training=True)\n",
    "            loss = self.loss_function(label, output)\n",
    "            \n",
    "        # update trainable variables\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "\n",
    "        # update metrics\n",
    "        self.metrics_list[1][0].update_state(tf.argmax(output, axis=1), tf.argmax(label, axis=1))\n",
    "        self.metrics_list[1][1].update_state(loss)\n",
    "        self.metrics_list[1][2].update_state(self.compute_frobenius())\n",
    "        \n",
    "        # return a dict with metric information\n",
    "        return {m.name : m.result() for m in self.metrics_list[1]}\n",
    "\n",
    "\n",
    "\n",
    "    # test_step method\n",
    "    @tf.function\n",
    "    def test_step(self, data):\n",
    "        img, label = data\n",
    "\n",
    "        # compute output and loss, without training\n",
    "        output = self(img, training=False)\n",
    "        loss = self.loss_function(label, output)\n",
    "\n",
    "        # update metrics\n",
    "        self.metrics_list[0][0].update_state(tf.argmax(output, axis=1), tf.argmax(label, axis=1))\n",
    "        self.metrics_list[0][1].update_state(loss)\n",
    "        self.metrics_list[0][2].update_state(self.compute_frobenius())\n",
    "\n",
    "        # return a dict with metric information \n",
    "        return {m.name : m.result() for m in self.metrics_list[0]}\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "id": "8svy39RCW_9o"
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_summary_writers(config_name):\n",
    "    \n",
    "    # Define where to save the logs\n",
    "    # along with this, you may want to save a config file with the same name so you know what the hyperparameters were used\n",
    "    # alternatively make a copy of the code that is used for later reference\n",
    "    \n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "    train_log_path = f\"logs/{config_name}/{current_time}/train\"\n",
    "    val_log_path = f\"logs/{config_name}/{current_time}/val\"\n",
    "\n",
    "    # log writer for training metrics\n",
    "    train_summary_writer = tf.summary.create_file_writer(train_log_path)\n",
    "\n",
    "    # log writer for validation metrics\n",
    "    val_summary_writer = tf.summary.create_file_writer(val_log_path)\n",
    "    \n",
    "    return train_summary_writer, val_summary_writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "id": "0R4loQRQW_9p"
   },
   "outputs": [],
   "source": [
    "# gets in a dataset and returns target values\n",
    "def prepare_data(dataset, seq_size, batch_size):\n",
    "\n",
    "    # convert data from uint8 to float32\n",
    "    dataset = dataset.map(lambda img, target: (tf.cast(img, tf.float32), target))\n",
    "    # convert image values from range [0, 255] to [-1, 1]\n",
    "    dataset = dataset.map(lambda img, target: ((img/128.)-1., target))\n",
    "\n",
    "    # Create random tuples of 4 images [[img, ...], [target, ...]]\n",
    "    dataset = dataset.batch(seq_size, drop_remainder=True)\n",
    "\n",
    "    #for a, b in dataset:\n",
    "      #print(list(b.as_numpy_iterator()))\n",
    "\n",
    "    for imgs, targets in dataset.take(1):\n",
    "      print(targets)\n",
    "      for target in targets:\n",
    "        print(target)\n",
    "\n",
    "    # Generate cumulative sum targets\n",
    "    one_hot_depth = (int) (9*(seq_size/2))\n",
    "\n",
    "    def generate_sums(elems):\n",
    "      signs = tf.constant([1 if i % 2 == 0 else -1 for i in range(len(elems))], dtype=tf.int64)\n",
    "      return tf.one_hot(tf.cumsum(tf.math.multiply(elems, signs)), depth=one_hot_depth)\n",
    "\n",
    "    dataset = dataset.map(lambda imgs, targets: (imgs, generate_sums(targets)))\n",
    "\n",
    "    for imgs, targets in dataset.take(1):\n",
    "      print(targets)\n",
    "      for target in targets:\n",
    "        print(target)\n",
    "    \n",
    "    dataset = dataset.cache()\n",
    "    dataset = dataset.shuffle(4096)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "id": "GbGZjolXZZGH"
   },
   "outputs": [],
   "source": [
    "# trains the model by iterating through the dataset and applying training_step method epochs time\n",
    "def training_loop(model, train_ds, test_ds, epochs, train_summary_writer, memory):\n",
    "    metrics = []\n",
    "\n",
    "    # iterate over epochs\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "\n",
    "        # train steps on all batches in the training data\n",
    "        for (img, label) in train_ds:\n",
    "            metrics = model.train_step((img, label))\n",
    "            \n",
    "            # keep data in summary with metrics\n",
    "            with train_summary_writer.as_default():\n",
    "                for metric in model.metrics_list[1]:\n",
    "                    tf.summary.scalar(f\"{metric.name}\", metric.result(), step=epoch)\n",
    "\n",
    "        for (key, value) in metrics.items():\n",
    "            memory[key].append(value.numpy())\n",
    "\n",
    "        memory = test_loop(model=model,\n",
    "                           test_ds=test_ds,\n",
    "                           val_summary_writer=val_summary_writer,\n",
    "                           memory=memory)\n",
    "        \n",
    "        # print current metric values and reset the metrics\n",
    "        tf.print([f\"{key} : {value.numpy()}\" for (key, value ) in metrics.items()])\n",
    "        model.reset_metrics(1)\n",
    "\n",
    "    return memory\n",
    "\n",
    "\n",
    "# tests overall performance of model\n",
    "def test_loop(model, test_ds, val_summary_writer, memory):\n",
    "    metrics = []\n",
    "    # test steps on every item in test dataset\n",
    "    for (img, label) in tqdm(test_ds):\n",
    "        metrics = model.test_step((img, label))\n",
    "        \n",
    "        # keep data with metrics\n",
    "        with val_summary_writer.as_default():\n",
    "            for metric in model.metrics_list[0]:\n",
    "                tf.summary.scalar(f\"{metric.name}\", metric.result(), step=1)\n",
    "\n",
    "    for (key, value) in metrics.items():\n",
    "        memory[key].append(value.numpy())\n",
    "\n",
    "    print([f\"{key} : {value.numpy()}\" for (key, value ) in metrics.items()])\n",
    "\n",
    "    model.reset_metrics(0)\n",
    "\n",
    "    return memory\n",
    "\n",
    "# visualize accuracy, loss and frobenius norm\n",
    "def visualization(accuracies, losses, frobenius, name):\n",
    "    plt.figure()\n",
    "    line1, = plt.plot(accuracies, \"b\")\n",
    "    line2, = plt.plot(losses, \"r\")\n",
    "\n",
    "    frob_new = frobenius/np.max(frobenius) * np.max(losses)\n",
    "    line3, = plt.plot(frob_new, \"y\" )\n",
    "\n",
    "    plt.xlabel(\"Training steps\")\n",
    "    plt.ylabel(\"Loss/Accuracy\")\n",
    "    plt.legend((line1, line2, line3),(\"Accuracy\", \"Loss\", \"Frobenius Norm\"))\n",
    "    plt.savefig(name)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "id": "dDCxrRjh_WI8"
   },
   "outputs": [],
   "source": [
    "# overall hyperparameters to compare with and without overfitting precautions methods\n",
    "epochs = 15\n",
    "batch_size = 32\n",
    "seq_size = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dvQPXaqlBYxF"
   },
   "source": [
    "## no augmentation, normal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "id": "gziNcUqxD-M3",
    "outputId": "8aa48802-e328-406b-f495-0c293be76466",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor([4 1 0 7], shape=(4,), dtype=int64)\n",
      "tf.Tensor(4, shape=(), dtype=int64)\n",
      "tf.Tensor(1, shape=(), dtype=int64)\n",
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(7, shape=(), dtype=int64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]], shape=(4, 18), dtype=float32)\n",
      "tf.Tensor([0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(18,), dtype=float32)\n",
      "tf.Tensor([0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(18,), dtype=float32)\n",
      "tf.Tensor([0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(18,), dtype=float32)\n",
      "tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(18,), dtype=float32)\n",
      "tf.Tensor([2 0 4 8], shape=(4,), dtype=int64)\n",
      "tf.Tensor(2, shape=(), dtype=int64)\n",
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(4, shape=(), dtype=int64)\n",
      "tf.Tensor(8, shape=(), dtype=int64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]], shape=(4, 18), dtype=float32)\n",
      "tf.Tensor([0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(18,), dtype=float32)\n",
      "tf.Tensor([0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(18,), dtype=float32)\n",
      "tf.Tensor([0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(18,), dtype=float32)\n",
      "tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(18,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "train_summary_writer, val_summary_writer = create_summary_writers(config_name=\"UNTOUCHED\")\n",
    "\n",
    "model = BasicConv(seq_size)\n",
    "\n",
    "\n",
    "#for img, label in train_ds.take(1):\n",
    "#    print(img.shape)\n",
    "#    print(label.shape)\n",
    "\n",
    "train_dataset = prepare_data(train_ds, seq_size, batch_size)\n",
    "#train_dataset = train_ds.apply(lambda dataset: prepare_data(dataset, seq_size, batch_size))\n",
    "val_dataset = prepare_data(val_ds, seq_size, batch_size)\n",
    "#val_dataset = val_ds.apply(lambda dataset: prepare_data(dataset, seq_size, batch_size))\n",
    "\n",
    "#for img, label in train_dataset.take(1):\n",
    "#    print(img.shape)\n",
    "#    print(label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "id": "8G0PkfIFZZGM",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "02a6bca6-2f72-4d67-b018-ee537a778878"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "469/469 [==============================] - 12s 20ms/step - loss: 0.0383 - accuracy: 0.3030 - val_loss: 0.0381 - val_accuracy: 0.0843\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 4s 10ms/step - loss: 0.0380 - accuracy: 0.1819 - val_loss: 0.0380 - val_accuracy: 0.3568\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.0379 - accuracy: 0.1945 - val_loss: 0.0380 - val_accuracy: 0.3568\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.0379 - accuracy: 0.2256 - val_loss: 0.0380 - val_accuracy: 0.2020\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 4s 10ms/step - loss: 0.0379 - accuracy: 0.2098 - val_loss: 0.0379 - val_accuracy: 0.2043\n",
      "Epoch 6/20\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.0378 - accuracy: 0.2231 - val_loss: 0.0379 - val_accuracy: 0.3194\n",
      "Epoch 7/20\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.0378 - accuracy: 0.2534 - val_loss: 0.0379 - val_accuracy: 0.1989\n",
      "Epoch 8/20\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.0378 - accuracy: 0.2524 - val_loss: 0.0379 - val_accuracy: 0.3160\n",
      "Epoch 9/20\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.0378 - accuracy: 0.2738 - val_loss: 0.0379 - val_accuracy: 0.3194\n",
      "Epoch 10/20\n",
      "469/469 [==============================] - 4s 10ms/step - loss: 0.0378 - accuracy: 0.2596 - val_loss: 0.0379 - val_accuracy: 0.3160\n",
      "Epoch 11/20\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.0378 - accuracy: 0.2890 - val_loss: 0.0379 - val_accuracy: 0.3194\n",
      "Epoch 12/20\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.0378 - accuracy: 0.2869 - val_loss: 0.0379 - val_accuracy: 0.3194\n",
      "Epoch 13/20\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.0378 - accuracy: 0.2631 - val_loss: 0.0379 - val_accuracy: 0.3194\n",
      "Epoch 14/20\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.0378 - accuracy: 0.2686 - val_loss: 0.0379 - val_accuracy: 0.3194\n",
      "Epoch 15/20\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.0378 - accuracy: 0.2813 - val_loss: 0.0379 - val_accuracy: 0.3194\n",
      "Epoch 16/20\n",
      "469/469 [==============================] - 4s 10ms/step - loss: 0.0378 - accuracy: 0.2685 - val_loss: 0.0379 - val_accuracy: 0.3214\n",
      "Epoch 17/20\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.0378 - accuracy: 0.2955 - val_loss: 0.0379 - val_accuracy: 0.1989\n",
      "Epoch 18/20\n",
      "469/469 [==============================] - 4s 10ms/step - loss: 0.0378 - accuracy: 0.2700 - val_loss: 0.0379 - val_accuracy: 0.3194\n",
      "Epoch 19/20\n",
      "469/469 [==============================] - 4s 9ms/step - loss: 0.0378 - accuracy: 0.2675 - val_loss: 0.0379 - val_accuracy: 0.3160\n",
      "Epoch 20/20\n",
      "469/469 [==============================] - 5s 10ms/step - loss: 0.0378 - accuracy: 0.2896 - val_loss: 0.0379 - val_accuracy: 0.3194\n",
      "Model: \"basic_conv_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " max_pooling2d_30 (MaxPoolin  multiple                 0 (unused)\n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_60 (Conv2D)          multiple                  160       \n",
      "                                                                 \n",
      " conv2d_61 (Conv2D)          multiple                  2320      \n",
      "                                                                 \n",
      " time_distributed_47 (TimeDi  multiple                 0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " conv2d_62 (Conv2D)          multiple                  4640      \n",
      "                                                                 \n",
      " conv2d_63 (Conv2D)          multiple                  9248      \n",
      "                                                                 \n",
      " time_distributed_48 (TimeDi  multiple                 0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_49 (TimeDi  multiple                 330       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " rnn_12 (RNN)                multiple                  608       \n",
      "                                                                 \n",
      " dense_83 (Dense)            multiple                  162       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17,468\n",
      "Trainable params: 17,468\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Test compile\n",
    "#print(train_dataset)\n",
    "#for imgs, targets in train_dataset.take(1):\n",
    "#  print(targets)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(), \n",
    "              loss='mean_squared_error', \n",
    "              metrics=[\n",
    "                      tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
    "                      #tf.keras.metrics.Mean(name=\"frob_norm\")\n",
    "                      ]\n",
    "              )\n",
    "\n",
    "history = model.fit(train_dataset,\n",
    "                    validation_data=val_dataset,\n",
    "                    epochs=20)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "id": "Yd8MuzB_-7mb",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "outputId": "04a3801f-eeba-4ea3-ed90-fe30e242034a"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\ntfds.benchmark(train_dataset, batch_size=batch_size)\\n\\nprint(\"\\n************ Training UNTOUCHED ************\\n\")\\n\\nmemory = {\"test_accuracy\" : [],\\n          \"test_loss\" : [],\\n          \"test_frob_norm\" : [],\\n          \"train_accuracy\" : [],\\n          \"train_loss\" : [],\\n          \"train_frob_norm\" : []\\n          }\\n\\nmemory = training_loop(model,\\n                       train_ds=train_dataset,\\n                       test_ds=val_dataset,\\n                       epochs=tf.constant(epochs),\\n                       train_summary_writer=train_summary_writer,\\n                       memory=memory)\\n\\nvisualization(memory[\"train_accuracy\"], memory[\"train_loss\"], memory[\"train_frob_norm\"], \"aug_train\")\\n'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 175
    }
   ],
   "source": [
    "\"\"\"\n",
    "tfds.benchmark(train_dataset, batch_size=batch_size)\n",
    "\n",
    "print(\"\\n************ Training UNTOUCHED ************\\n\")\n",
    "\n",
    "memory = {\"test_accuracy\" : [],\n",
    "          \"test_loss\" : [],\n",
    "          \"test_frob_norm\" : [],\n",
    "          \"train_accuracy\" : [],\n",
    "          \"train_loss\" : [],\n",
    "          \"train_frob_norm\" : []\n",
    "          }\n",
    "\n",
    "memory = training_loop(model,\n",
    "                       train_ds=train_dataset,\n",
    "                       test_ds=val_dataset,\n",
    "                       epochs=tf.constant(epochs),\n",
    "                       train_summary_writer=train_summary_writer,\n",
    "                       memory=memory)\n",
    "\n",
    "visualization(memory[\"train_accuracy\"], memory[\"train_loss\"], memory[\"train_frob_norm\"], \"aug_train\")\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "ki_uebungen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f29be5cdbd889ccae82f59fdf4d8dd89ee7979a66140a153cd0c4d342f0eb10a"
   }
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
